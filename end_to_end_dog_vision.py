# -*- coding: utf-8 -*-
"""end-to-end-dog-vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GDehywn83TUG20T_yTBkMMhsldGCmWEE

# End to end multi-class Dog Breed Classffication
This notebook builds an end-to-end multi-class image classifier using Tensorflow 2.0 ans Tensorflow Hub
## 1.Problem
Identifying the breed of a dog given an image of a dog.
When I'm sitting at the cafe and I take a photo of a dog, I want to know what breed it is.
## 2. Data

The data we're using is from kaggle's dog breed identification competetion.

## 3. Evaluation

The evaluation is a file with prediction probabilities for each dog breed of each test image
https://www.kaggle.com/c/dog-breeidentificationd-/overview/evaluation

## 4. Features

Some information about the data:
* We're dealing with images (unstructured data) so it's probably best we use deep learning/transfer learning.
* There are 120 breeds of dogs (this means there are 120 different classes).
* There are around 10,000+ images in train set (these images have labels)
* The are around 10,000+ images in the test set(these images have labels, because we'll want to predict them
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Getting workspace ready
* Import Tensorflow
* Import Tensorflow Hub
* Make sure using a GPU
"""

# Import tensorflow
import tensorflow as tf
import tensorflow_hub as hub
print('TF version:', tf.__version__)
print('TF Hub version:', hub.__version__)

# Check for GPU availability
print("GPU", "available (YESSSSSSS!)" if tf.config.list_physical_devices("GPU") else "not avail")

"""## Getting our data ready (turning into tensors)
With all machine leanring models, our data has to be numerical format. So that's what we'll be doing first. Turing our images into tensors (numerical representation).

Let's start by accessing our data and checking out the labels.

"""

# Checkout the labels of our data
import pandas as pd
labels_csv = pd.read_csv('drive/MyDrive/dog vision/labels.csv')
print(labels_csv.describe())
print(labels_csv.head())

labels_csv.head()

# How many images of each breed are there
labels_csv['breed'].value_counts().plot.bar(figsize=(20, 10))

labels_csv['breed'].value_counts().median()

# Let's view an image
from IPython.display import Image
Image("drive/MyDrive/dog vision/train/000bec180eb18c7604dcecc8fe0dba07.jpg")

"""## Getting images and their labels 
Let's get a list of all image file pathnames.
"""

labels_csv.head()

# Create pathnames from image ID's
filenames = ['drive/MyDrive/dog vision/train/'+ fname + ".jpg" for fname in labels_csv['id']]
filenames[:10]

# Check whether no.of filenames matches no.of actual image files
import os
if len(os.listdir("drive/My Drive/dog vision/train/")) == len(filenames):
  print('Filenames match actual amount of files. Proceed')
else:
  print('Filenames do not match. check the target')

# One more check 
Image(filenames[10000])

labels_csv['breed'][10000]

"""Since we've got our training image paths in a list, let's prepare out labels"""

import numpy as np
labels = labels_csv['breed']
labels = np.array(labels)
labels

len(labels)

# See if number of labels matches the number of filenames
if len(labels) == len(filenames):
  print('Number of labels matches number of filenames!')
else:
  print('Number of labels does not match number of filenames, check data directories')

# Find the unique label values
unique_breeds = np.unique(labels)
unique_breeds

len(unique_breeds)

# Turn a single label into an array of booleans
print(labels[0])
labels[0] == unique_breeds

# Turn every label into a boolean array
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

len(boolean_labels)

# Example Turning boolean array into integers
print(labels[0])
print(np.where(unique_breeds == labels[0])) # index where label occurs
print(boolean_labels[0].argmax()) # index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # this will be a 1 where the sample label occurs

filenames[:10]

boolean_labels[:2] #his article will get you started with Google Colab, a free GPU cloud service with an editor based on Jupyter No

"""### Creating our own validation set
Since the dataset from Kaggle doesn't come with a validation set , we're going create our own
"""

# Setup x & y variables
X = filenames
y = boolean_labels

len(filenames)

"""We're going to start off experimenting with ~1000 images and increase as needed."""

# Set number of images to use for experimenting
NUM_IMAGES = 1000 #@param {type:'slider', min:1000, max:10000, step:100}

# Let's split our data into train and validation sets
from sklearn.model_selection import train_test_split

np.random.seed(42)

# Split them into training and validation of total size NUM_IMAGES
X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size=0.2,
                                                  random_state=42)

len(X_train), len(y_train), len(X_val), len(y_val)

# Let's have a look at the training data
X_train[:5], y_train[:2]

"""## Preprocessing Images (turning images into Tensors)
To preprocess our images into Tensors we're going to write a function which does a few things:
1. Take an image filepath as input 
2. Use Tensorflow to read the file and save it to a vairable, `image`
3. Turn our `image` (a jpg) into Tensors
4. Resize the `image` to be shape of (224, 224)
5. Return the modified `image`

"""

# Convert image into a numpy array
from matplotlib.pyplot import imread
image = imread(filenames[42])
image.shape

image.max(), image.min()

# turn image into tensor
tf.constant(image)[:2]

"""Now we've seen what an image looks like as a Tensor, let's make a function to preprocess them
## Preprocessing Images (turning images into Tensors)
To preprocess our images into Tensors we're going to write a function which does a few things:
1. Take an image filepath as input 
2. Use Tensorflow to read the file and save it to a vairable, `image`
3. Turn our `image` (a jpg) into Tensors
4.  Normalize our image (convert color channel values from 0-255 to 0-1)
5. Resize the `image` to be shape of (224, 224)
6. Return the modified `image`

"""

# Define image size
IMG_SIZE = 224
# Create a fucntion for preprocessing images
def process_image(image_path, img_size=IMG_SIZE):
  # Read in an image file
  image = tf.io.read_file(image_path)
  # Turn the jpeg image into numerical tensor with 3 color channels  (Red, green, blue)
  image =  tf.io.decode_jpeg(image, channels=3)
  # Convert the color channe values from 0-255 t0 to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image to our desired value 
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  
  return image

tensor = tf.io.read_file(filenames[34])
tensor

tensor =  tf.io.decode_jpeg(tensor, channels=3)
tensor

tf.image.convert_image_dtype(image, tf.float32)

"""## Turning our data into batches
Why turn our data into bathces

Let's say you're trying to process 10000+ images in one go.. they all might not fit into memory

So that's why we do about 32 (This is the batch size)
images at a time (you can manually adjust the batch size if needed).

In order to use Tensorflow effectively, we need our data in the form of Tensor tuples which look like this:
`(image, label)`

"""

# Create a simple fucntion to return a tuple (image, label)
def get_image_label(image_path, label):
  """ Takes an image file path name and the associated label,
  processes the image and returns a tuple of (image, label)
  """
  image = process_image(image_path)
  return image, label

# Demo of the image
(process_image(X[42], y[42]))

"""Now we've got a wat to turn our data into tuples of Tensors in the form : `(image, label)`, let's make a funtion to turn all our data (x & y) into batches"""

# Define the vatch size, 32 a good start
BATCH_SIZE = 32
# Create a fucntion to turn data to bathces
def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
  Create batches of data out of image (X) and label (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle if its validation data
  Also accepts test data as input (no labels)
  """

  # if the data is test datasrt, wr probably dont have labels
  if test_data:
    print('Creating the data batches...')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))
    data_batch =  data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  # if the data is valid dataset, we don't need to shuffle it
  elif valid_data:
    print('Creating validation data batches...')
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths
                                              tf.constant(y)))  # labels
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print('Creating training data batches...')
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                               tf.constant(y)))
    # shuffling pathnames and labels before mapping image processor funtion is faster than shuffling images
    data = data.shuffle(buffer_size=len(X))
    # create image label tuples this alos turn the image path into preprocesse image
    data = data.map(get_image_label)

    # turn the training data into batches
    data_batch = data.batch(BATCH_SIZE)

  return data_batch

# Create training and valodation data batches
train_data = create_data_batches(X_train, y_train)
val_data  = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attribute of our data bathces
train_data.element_spec, val_data.element_spec

"""## Visulizing Data Batches
Our data is now in batches, however, these can be a little hard to understand/comprehend, let's visualize them
"""

import matplotlib.pyplot as plt
# Create a fucntion for viewing images in a data batch
def show_25_images(images, labels):
  """
  Displays a plot of 25 images and their labels from a data batch
  """
  # setup the figure
  plt.figure(figsize=(10, 10))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Create subplots (5 rows, 5 columns)
    ax = plt.subplot(5, 5, i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image label as title
    plt.title(unique_breeds[labels[i].argmax()])
    # turn the gridlines off
    plt.axis('off')

train_data

train_images, train_labels = next(train_data.as_numpy_iterator())
len(train_images), len(train_labels)

# Now let's visualize the data in a training batch
show_25_images(train_images, train_labels)

val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""## Building a model
Before we build a model, there are a few things we need to define
* The `input` shape (our images shape, in the form of Tensors) to our model.
* The ouput shape (image labels, in the form of Tensors) of our model.
* The URL of the model we want to use is from tensorflow hub.
* https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5
* Here I choose to use transfer learning. So I took a model from tensorflow hub
* The model we're gonna use is mobilenet_v2_130_224/classification/5
"""

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch , height , width, colour channels

# Setup output shape of our model
OUTPUT_SHAPE = len(unique_breeds)

#Setup model URL from Tensorflow Hub
MODEL_URL =  'https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5'

"""Now we've got our inputs, outputs and model ready to go.
Let's put them together into a Keras deep learning model

Creating a function which:

* Defines the layers in a Keras model in sequential fashion.

* Compiles the model

* Builds the model



---


"""

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print('Building model with:', MODEL_URL)

  # Setup the model layers
  model = tf.keras.Sequential([
       hub.KerasLayer(MODEL_URL),# Layer1 input layer
       tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                              activation='softmax') # Layer 2 output layer
  ])

  # Compile the model
  model.compile(
      loss = tf.keras.losses.CategoricalCrossentropy(),
      optimizer = tf.keras.optimizers.Adam(),
      metrics = ['accuracy']
  )

  # Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

outputs = np.ones(shape=(1, 1, 1280))
outputs

"""## Creating callbacks
Callbacks are helper functions can use during training to do such things as save its progress, check its progress or stop training early if a model starts improving.

We'll create two callbacks, ine for Tensorboard which helps and another for early stopping  which prevents long time training of the model

## Tensorboard Callback

To setup a tensorboard callback, we nned to do 3 things.
1. Load the extension 
2. Create a Tensorboard callback which is able save to logs to a directory and pass it to our model's `fit()` fucntion.
3. Visualize our models training logs with the %tensorboard magic function (after model training)
"""

# Commented out IPython magic to ensure Python compatibility.
# Load Tensorboard notebook extension
# %load_ext tensorboard

import datetime
import os

# Create a function to build a tensotboard callback
def create_tensorboard_callback():
  # Create a log directory for storing Tensorboard logs
  logdir = os.path.join('drive/MyDrive/dog vision/logs',
                        # Make it so the logs get tracked whenever we run an expreiment
                        datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
  return tf.keras.callbacks.TensorBoard(logdir)

tensorboard = create_tensorboard_callback()

"""### Early stopping callback

Early stopping helps to stop our model from overfitting by stopiing if a certain value metric stops improving

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping

"""

# Create early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
                                                  patience=3)

"""## Training a model (on subset of data)

Our first model is only going to train on 1000 images, to make sure it's working
"""

NUM_EPOCHS = 100 #@param {type:'slider', min:10, max:100, step:10}

"""Let's creste a function which trains a model

* Create a model using `create_model()`
* Setup a Tensorboard callback using `create_tensorboard_callback()`
* Call the `fit()` function on our model passing it the training data, validation data, number of epochs to train for (`NUM_EPOCHS`) and the callbacks we like to use
* Return the model
"""

# Build a function to train and return a trained model
def train_model():
  """
  Trains a given model and returns the trained version.
  """

  # create a  mode
  model = create_model()

  # crete a new Tensorboard session everytime we train a model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the data
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  
  return model

# Fit the model to the data
model = train_model()
model

"""Looks like our model is overfitting as we can see that it's performing way better on training data than validation data
we'll use some ways to prevent overfitting

## checking the tensorboard logs

The TensorBoard magic function (`%tensorboard`) will access the logs directory we created earlier and visualize contents
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive//dog\ vision/logs

"""## Making and evaluating predictions using a trained model"""

val_data

predictions = model.predict(val_data, verbose=1)
predictions

# First prediction
index = 56
print(predictions[0])
print(f'max value (probability of prediction: {np.max(predictions[index])}')
print(f'sum: {np.sum(predictions[index])}')
print(f'Max index: {np.argmax(predictions[index])}')
print(f'Predicted label: {unique_breeds[np.argmax(predictions[index])]}')

unique_breeds[17]

"""Having the above functionality is great but we want to be able to do it scale.

And it would be even better if we could see the image the prediction is being made on

*Note** : Prediction probablities are also known as confidence levels.
"""

# Turn predcition probablities into their respective labels
def get_pred_label(prediction_probabilities):
  """
  Turns an array of prediction probabilities into a label
  """
  return unique_breeds[np.argmax(prediction_probabilities)]

# Get a predicted label based on array of prediction probabilities
pred_label = get_pred_label(predictions[119])
pred_label

"""Now since our validation data is still in batch dataset, we'll have to unbatch it to make predictions on the validation images and then compare those predictions to the validation labels(truth labels)."""

val_data

# Create a function to unbatch a batch dataset
def unbatchify(data):
  """
  Takes a batched dataset of (image, label) Tensors and returns separate arrays
  of images and labels.
  """
  images_ = []
  labels_ = []

  for image, label in data.unbatch().as_numpy_iterator():
    images_.append(image)
    labels_.append(unique_breeds[np.argmax(label)])
  return images_, labels_

# Unbatchify the validation data
val_images , val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

get_pred_label(val_images[0])

"""Now we've got ways to get:

* Prediction labels
* Validation labels(truth labels)
* Validation images

Let's make some fucntion to make these all a bit visualize

We'll create a function which:

* Takes an array of prediction probabilities , an array of truth labels and an array of images and integers.
* Convert the prediction probabilities to a predicted label.
* Plot the predicted labels, its predicted probability,
the truth label and the target image on  a single plot.
"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction, ground truth and image for sample n
  """
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the colour of the title depending on if the prediction 
  if pred_label == true_label:
    color = 'green'
  else:
    color = 'red'
  
  # Change plot title to be predicted, probability of prediction and truht label
  plt.title('{} {:2.0f}% {}'.format(pred_label,
                                     np.max(pred_prob)*100,
                                     true_label),
                                     color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=88)

"""Now we've got one fucntion to visualoze our models top prediction, let's make to view our models top 10 predict

This fucntion will:
* Take an input of prediction probabilities array and a ground truth array and an integer
* Find the predcition using `get_pred_label()`
* Find the top 10:
   * Prediction probabilites indexes
   * Prediction probabilities values
   * Prediction labels
* Plot the top 10 prediction probability values and labels, colouring the true label green
"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # finr the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]

  # find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color='grey')
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation='vertical')
  
  # change the colour of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color('green')
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)

"""Now we've got some function to help us visualize our predictions and evalauate our model"""

# Let's check out a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range (num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)

plt.tight_layout(h_pad=1.0)
plt.show()

"""## Saving and reloading a trained model"""

# Create a function to save a model
def save_model(model, suffix=None):
  """
  saves a given model in models directory and appends a suffix
  """
  # Create a model directory pathname with current time
  modeldir = os.path.join('drive/MyDrive/dog vision/models',
                          datetime.datetime.now().strftime('%Y%m%d-%H%M%s'))
  model_path = modeldir + '-' + suffix + '.h5'
  print(f'Saving model to: {model_path}...')
  model.save(model_path)
  return model_path

# Create a function to load the trained model
def load_model(model_path):
  """
  Loads a saved model from a specified path
  """
  print(f'Loading saved model from: {model_path}')
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={'KerasLayer':hub.KerasLayer})
  return model

"""Now we've got functions to save and load a trained model, let's make sure they work"""

# Save our model trained on 100 images
save_model(model, suffix='1000-images-mobilenetv2-Adam')

# Load a trained model
loaded_1000_image_model = load_model('drive/MyDrive/dog vision/models/20210523-17271621790837-1000-images-mobilenetv2-Adam.h5')

# Evaluate the pre-saved model
model.evaluate(val_data)

loaded_1000_image_model.evaluate(val_data)

"""## Training a big dog model (on the full data)"""

len(X), len(y)

X[:10]

# Create a data batch with the full data batches
full_data = create_data_batches(X, y)

full_data

# Create a model for full model
full_model = create_model()

# Create full model callbacks
full_model_tensorboard = create_tensorboard_callback()
# No validation set when training on all the data, so we can't monitor validation accuracy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy',
                                                             patience=3)

"""**Note:** Runnig the cell below will take a little while
(may be 30min)
"""

# Fit the full model to full data
full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard, full_model_early_stopping])

"""Output was omitted to avoid recursion"""

save_model(full_model, suffix='full-image-set-mobilenetv2-Adam')

"""output was omitted to avoid recursion"""

import tensorflow as tf
from tensorflow import keras

"""## Loading a saved model"""

full_loaded_model = load_model('drive/MyDrive/dog vision/models/20210523-19351621798537-full-image-set-mobilenetv2-Adam.h5')

"""## Making predictions on test images
Since our model has been trained on images in the orm of Tensor batches, to make prediction on the test data, we'll have to get it into same format.

Luckily we created `create_data_batches()` earlier which can take a list of filenames as input and convert them into Tensor batches

To make predictions on the test data , we'll:
* Get the test image filenames
* Convert the filnames into test data batches using `create_data_batches()` and setting the 'test_data` parameter to `True` (since the test data does'nt have labesl.

* Make a predcitions array passing thr batches to the predict() method called on model


"""

# Load the test image filenames
test_path = 'drive/MyDrive/dog vision/test/'
test_filenames = [test_path + fname for fname in os.listdir(test_path)]
test_filenames[:10]

len(test_filenames)

# Create test data batch
test_data = create_data_batches(test_filenames, test_data=True)

test_data

"""**NOTE:** Calling predict() on our full model and passing it the test data batch will take a long time to run"""

# MAke predictions  on test data bathc using the loaded_full_model
test_prediction = full_loaded_model.predict(test_data,
                                            verbose=1)

# Save predictions (Numpy array) to csv file (for access later)
np.savetxt('drive/MyDrive/dog vision/preds_array_csv', test_prediction, delimiter=',')

# Load predictions (Numoy array) from csv file
test_predictions = np.loadtxt('drive/MyDrive/dog vision/preds_array_csv', delimiter=',')

test_predictions[:10]

test_predictions.shape

"""## Preparing test dataset predictions for kaggle
Looking at the kaggle sample submission , we find that it wants our models prediction probabaility outputs in a
DataFrame with an ID and a column for each different dog breed.

To get the data in this format, we'll:
* Create a pandas DataFrame with an ID column as well as a column for each dog breed
* Add data to the ID column by extracting the test image ID's from their filepaths.
* Add data (the prediction probabilities) to each of the dog breed columns
* Export the DataFrame as a CSV to submit it to Kaggle
"""

['id'] + list(unique_breeds)

import pandas as pd
# Create a pandas DataFrame with empty columns
pred_df = pd.DataFrame(columns=['id'] + list(unique_breeds))

pred_df.head()

test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
pred_df['id'] = test_ids

os.path.splitext(test_filenames[0])

pred_df.head()

# Add the prediction probabilities to each dog breed column
pred_df[list(unique_breeds)] = test_predictions
pred_df.head()

# Save our predictions dataframe to csv for submission to Kaggle
pred_df.to_csv('drive/MyDrive/dog vision/full_model_predictions_submission_1_mobilev2.csv',
               index=False)

"""## Making predictions on custom images

To make predictions on custom images, we'll:

* Get the filepaths of our own images.
* Turn the filepaths into data batches using `create_data_batches()`. And since our custom images won't have labels, we set the  `test_data` parameter to  `True`.
* Pass the custom image data batch to our model's `predict()` method.
* Convert the prediction output probablities to predictions label
* Compare the predicted labels to the custom images.
"""

# Get the custom image filepaths
custom_path = 'drive/MyDrive/dog vision/dog photos/'
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

custom_image_paths

# Turn custom images into data batches
custom_data  = create_data_batches(custom_image_paths, test_data=True)
custom_data

custom_preds = full_loaded_model.predict(custom_data)

custom_preds.shape

# Get custom image prediction labels 
custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Get custom images (batchify() won't work bcoz  there are no labels)
custom_images = []
# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions 
plt.figure(figsize=(10,10))
for i, image in enumerate(custom_images):
  ax = plt.subplot(3,3,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)

